# SEBI RAG Chatbot - Complete Implementation Guide

## âœ… What the Scraper Now Does

### 1. **Recursive Document Downloading**
- Downloads all Master Circulars from SEBI website
- Extracts text from each PDF to find referenced circulars
- Searches for and downloads referenced documents automatically
- Goes 2 levels deep by default (original â†’ references â†’ references of references)

### 2. **Smart Pattern Matching**
Detects SEBI circular patterns:
- `SEBI/HO/IMD/IMD_II/P/CIR/2024/123`
- `SEBI/HO/CFD/DIL2/CIR/P/2024/0112`
- `Circular No. XXX`
- `Notification No. XXX`
- And more variations

### 3. **Metadata & Tracking**
- Saves metadata for each document (title, date, URL, circular number)
- Tracks downloaded documents to avoid duplicates
- Creates a reference graph showing document relationships
- Resumes from where it left off if interrupted

### 4. **Organized Output**
```
sebi_master_circulars/     # All PDF files
sebi_metadata/             # Metadata JSON files
  â”œâ”€â”€ downloaded_documents.json    # Tracking file
  â”œâ”€â”€ document_references.json     # Reference graph
  â””â”€â”€ [document_name].json         # Individual metadata
```

---

## ðŸ“‹ Additional Considerations for Your RAG System

### Phase 1: Installation & Scraping âœ…

```powershell
# Install dependencies
pip install -r requirements.txt

# Run the scraper
python sebi_downloader.py
```

---

### Phase 2: Document Processing

#### A. **Handle Scanned PDFs (OCR)**
Some older SEBI documents might be scanned images. You'll need OCR:

```python
# Install: pip install pytesseract pdf2image pillow
# Also install Tesseract: https://github.com/tesseract-ocr/tesseract

from pdf2image import convert_from_path
import pytesseract

def extract_text_with_ocr(pdf_path):
    """Extract text from scanned PDFs"""
    images = convert_from_path(pdf_path)
    text = ""
    for image in images:
        text += pytesseract.image_to_string(image)
    return text
```

#### B. **Document Chunking Strategy**
For RAG, you need to split documents into chunks:

```python
# Recommended chunking approaches:
# 1. Semantic chunking (by sections/paragraphs)
# 2. Fixed-size with overlap (e.g., 500 tokens, 50 overlap)
# 3. Sentence-based chunking

# Example with LangChain:
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
)
```

#### C. **Extract Tables and Structured Data**
SEBI documents have many tables (like the deposit table in your screenshot):

```python
# pdfplumber can extract tables
import pdfplumber

with pdfplumber.open("document.pdf") as pdf:
    for page in pdf.pages:
        tables = page.extract_tables()
        # Convert to text or structured format
```

---

### Phase 3: Expand Document Coverage

Your scraper currently gets Master Circulars. Consider adding:

#### A. **Other SEBI Document Types**
```python
# Add these URLs to scrape:
urls_to_add = {
    "Circulars": "https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=1&smid=0",
    "Notifications": "https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=2&smid=0",
    "Press Releases": "https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=3&ssid=17&smid=0",
    "Orders": "https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=2&ssid=0&smid=0",
}
```

#### B. **Date Filtering**
Add date range filtering to keep documents current:

```python
from datetime import datetime, timedelta

# Only get documents from last 5 years
cutoff_date = datetime.now() - timedelta(days=5*365)

# Filter in your scraping logic
if document_date > cutoff_date:
    download_document()
```

---

### Phase 4: RAG Implementation

#### A. **Choose Vector Database**
Options for storing embeddings:
- **ChromaDB** - Simple, local, good for prototypes
- **Pinecone** - Managed, scalable
- **Weaviate** - Open-source, feature-rich
- **FAISS** - Facebook's library, fast

```python
# Example with ChromaDB:
import chromadb
from chromadb.config import Settings

client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db"
))

collection = client.create_collection(
    name="sebi_documents",
    metadata={"description": "SEBI regulatory documents"}
)
```

#### B. **Choose Embedding Model**
- **OpenAI embeddings** (text-embedding-3-large) - High quality, paid
- **Sentence Transformers** (all-MiniLM-L6-v2) - Free, good quality
- **BGE embeddings** (BAAI/bge-large-en) - State-of-the-art, free
- **Cohere embeddings** - Good for semantic search

```python
# Example with Sentence Transformers:
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(text_chunks)
```

#### C. **Metadata Filtering**
Use the metadata you're collecting for filtered search:

```python
# Search only in documents from 2024
results = collection.query(
    query_embeddings=query_embedding,
    where={"date": {"$gte": "2024-01-01"}},
    n_results=5
)

# Search only Investment Advisers circulars
results = collection.query(
    query_embeddings=query_embedding,
    where={"title": {"$contains": "Investment Adviser"}},
    n_results=5
)
```

#### D. **Hybrid Search**
Combine semantic search with keyword search:

```python
# Use BM25 for keyword search
from rank_bm25 import BM25Okapi

# Combine scores from:
# 1. Vector similarity (semantic)
# 2. BM25 (keyword)
# 3. Metadata relevance
# 4. Document recency

final_score = (0.6 * semantic_score + 
               0.3 * bm25_score + 
               0.1 * recency_score)
```

---

### Phase 5: Advanced RAG Features

#### A. **Citation & Source Tracking**
Use the reference graph you're building:

```python
# When returning an answer, include:
# 1. Source document(s)
# 2. Page numbers
# 3. Direct quotes
# 4. Related/referenced documents

response = {
    "answer": "...",
    "sources": [
        {
            "title": "Master Circular for Research Analysts",
            "date": "Feb 06, 2026",
            "page": 7,
            "quote": "...",
            "url": "...",
            "related_docs": ["SEBI/HO/IMD/..."]
        }
    ]
}
```

#### B. **Handle Cross-References**
Use your reference graph to:
- Show users related regulations
- Provide context from referenced documents
- Build knowledge graphs

#### C. **Update Mechanism**
SEBI publishes new documents regularly:

```python
# Schedule daily/weekly runs
# 1. Check for new documents
# 2. Re-extract references from updated docs
# 3. Update vector database
# 4. Version control for regulations

# Mark superseded documents
metadata["superseded_by"] = "SEBI/HO/CFD/2024/XYZ"
```

---

### Phase 6: LLM Integration

#### A. **Prompt Engineering**
```python
prompt_template = """
You are a SEBI regulatory expert. Answer questions based ONLY on the provided context.
If you're not certain, say so. Always cite the source circular/notification.

Context from SEBI documents:
{context}

User Question: {question}

Instructions:
1. Provide accurate information based on the context
2. Cite the circular number and date
3. If multiple documents are relevant, mention all
4. If context is insufficient, ask for clarification

Answer:
"""
```

#### B. **Choose LLM**
- **GPT-4** - Best quality, expensive
- **GPT-3.5-turbo** - Good balance
- **Claude** - Great for long contexts
- **Llama 2/3** - Free, self-hosted
- **Mistral** - Good open-source option

#### C. **Context Window Management**
- Most LLMs have token limits
- Implement re-ranking to get most relevant chunks
- Use map-reduce for multi-document questions

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True
)
```

---

### Phase 7: Evaluation & Quality

#### A. **Create Test Questions**
Build a test set of questions and expected answers:

```python
test_set = [
    {
        "question": "What are the deposit requirements for Research Analysts with 151-300 clients?",
        "expected_source": "Master Circular for Research Analysts",
        "expected_answer_contains": ["2 lakh", "deposit"]
    },
    # Add 50-100 questions
]
```

#### B. **Metrics to Track**
- **Answer correctness** - Manual evaluation
- **Source accuracy** - Did it cite the right document?
- **Retrieval precision** - Are retrieved chunks relevant?
- **Response time** - User experience metric
- **Citation accuracy** - Are references correct?

#### C. **Quality Checks**
```python
# Automated checks:
# 1. Does answer contain source citation?
# 2. Is source document in retrieved chunks?
# 3. Does answer contradict any context?
# 4. Hallucination detection
```

---

## ðŸš€ Recommended Tech Stack

### Minimal Setup (Prototype)
```
- PDF Processing: pdfplumber
- Embeddings: sentence-transformers
- Vector DB: ChromaDB
- LLM: GPT-3.5-turbo (via API)
- Framework: LangChain
```

### Production Setup
```
- PDF Processing: pdfplumber + pytesseract (OCR)
- Embeddings: OpenAI text-embedding-3-large
- Vector DB: Pinecone or Weaviate
- LLM: GPT-4 or Claude
- Framework: LangChain or LlamaIndex
- Monitoring: LangSmith or Weights & Biases
- API: FastAPI
- Frontend: Streamlit or React
```

---

## ðŸ“Š Estimated Resource Requirements

### Storage
- PDFs: ~500MB - 2GB (depends on how many years)
- Embeddings: ~1-5GB
- Metadata: ~10-50MB

### Processing Time
- Initial scraping: 2-8 hours (depends on depth)
- Embedding generation: 1-3 hours
- Query response: <2 seconds

### Costs (monthly, for production)
- Vector DB (Pinecone): $70-200
- LLM API (OpenAI): $50-500 (depends on usage)
- Embeddings: $20-100
- Total: ~$150-800/month

---

## ðŸŽ¯ Next Steps

1. **Run the scraper**: `python sebi_downloader.py`
2. **Explore downloaded files**: Check `sebi_master_circulars/` and `sebi_metadata/`
3. **Review reference graph**: Look at `document_references.json` to see connections
4. **Implement text extraction**: Process PDFs into text chunks
5. **Set up vector database**: Choose and configure your DB
6. **Generate embeddings**: Create vector representations
7. **Build retrieval system**: Implement search
8. **Integrate LLM**: Connect to your chosen model
9. **Create interface**: Build chat UI
10. **Test & iterate**: Evaluate with real questions

---

## âš ï¸ Important Notes

### Legal Compliance
- Ensure you comply with SEBI's terms of service
- These are public documents, but verify usage rights
- Add rate limiting to be respectful of SEBI's servers

### Data Quality
- Some older PDFs may be scanned (need OCR)
- Circular numbers have inconsistent formats (handled by regex patterns)
- Documents may be superseded - track versions

### Limitations
- The scraper may miss some cross-references (e.g., verbal references without circular numbers)
- Some documents may be in non-PDF formats
- External links to other regulatory bodies won't be followed

---

## ðŸ” Missing Anything?

Here's what I think you should also consider:

1. **Amendments & Updates**: Track which circulars supersede others
2. **FAQs**: SEBI publishes FAQs - include these
3. **Enforcement Actions**: Include orders for context
4. **Consultation Papers**: For upcoming regulations
5. **Annual Reports**: For broader context
6. **External References**: RBI, MCA documents referenced by SEBI
7. **User Feedback Loop**: Allow users to report incorrect answers
8. **Document Versioning**: Some circulars get amended
9. **Multi-language Support**: If SEBI publishes in regional languages
10. **Compliance Calendar**: Extract important dates and deadlines

Let me know what you'd like to tackle next!
